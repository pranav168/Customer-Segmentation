# -*- coding: utf-8 -*-
"""Online Retail Customer Segmentation final .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U8ZOo4oyOrlLg0netJbIgK3oYx05YP8t

# <b><u> Project Title : Extraction/identification of major topics & themes discussed in news articles. </u></b>

## <b> Problem Description </b>

### In this project, your task is to identify major customer segments on a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.

## <b> Data Description </b>

### <b>Attribute Information: </b>

* ### InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.
* ### StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.
* ### Description: Product (item) name. Nominal.
* ### Quantity: The quantities of each product (item) per transaction. Numeric.
* ### InvoiceDate: Invice Date and time. Numeric, the day and time when each transaction was generated.
* ### UnitPrice: Unit price. Numeric, Product price per unit in sterling.
* ### CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.
* ### Country: Country name. Nominal, the name of the country where each customer resides.
"""

from google.colab import drive                                                  #mounting drive
drive.mount('/content/drive')

import pandas as pd                                                             #loading all the Required libraries
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from yellowbrick.cluster import SilhouetteVisualizer
from sklearn.metrics import silhouette_score, silhouette_samples
from sklearn.ensemble import IsolationForest  
import warnings                                                                 # Removing all those annoying Warnings
warnings.filterwarnings('ignore')
warnings.simplefilter('ignore')
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from mpl_toolkits import mplot3d
from pylab import rcParams                                                      
rcParams['figure.figsize'] = 30, 10
import scipy.cluster.hierarchy as sch

df= pd.read_excel('/content/drive/MyDrive/Copy of Online Retail.xlsx')          #Loading the Dataset

df.info()                                                                       #Data contains a lot of Nan

df.isna().sum()                                                                 #135080 nulls in Customer id and 1454 nulls in Description

df.head()

column_list=['InvoiceNo','StockCode','Description','Quantity','InvoiceDate','UnitPrice','CustomerID','Country']
count_dataset=pd.DataFrame()
distinct_features=[]                                                                                          #Empty list to know the number of distict features,sum of all these values, and sum of values top 10 comprises
for i in column_list:                                                                                               
  count_dataset[i]= pd.Series(df[i].value_counts().sort_values(ascending=False).head(10).index)      
  count_dataset[f'{i}_count']=pd.Series(df[i].value_counts().sort_values(ascending=False).head(10).values).astype('int')   
  distinct_features.append((len(df[i].value_counts().index),df[i].value_counts().sum(),df[i].value_counts().sort_values(ascending=False).head(10).sum())) 
final_tally=list(zip(column_list,distinct_features))                                                          #Zipping with column_list
col_ref={}  
for i in column_list:
  if i in ['CustomerID','Description']:                                                                       #colur red shows the  Variable with missing values
    col_ref[i]='background-color: red'  
  else:
    col_ref[i]='background-color: blue'                                                                       #colur blue shows the features 
  temp=f'{i}_count'
  col_ref[temp]='background-color: green'                                                                     #colur green shows the count
def Nan_as_black(val):
  if str(val)=='nan':
    color = 'black'
    return 'color: %s' % color
count_dataset=count_dataset.style.apply(lambda x: pd.DataFrame(col_ref, index=count_dataset.index, columns=count_dataset.columns).fillna(''), axis=None).highlight_null('black').applymap(Nan_as_black)
count_dataset

final_tally                                                                     # Gives (No. of unique elements in Variable, Total of all Variable, Total of Top 10 Variable )

"""here important thing to note is that :


1.   Only 25900 unique Invoice
2.   Only 4070 unique StokeCode(which means number of products =4070)
3.   There are only 4372 Unique Customers (thus we should look a way to group data as per customers, thus we will have to treat the customers with Nan values as similar.
4.  only 38 Country of which 10 country represents almost entire data, and UK alone explain 90% Data thus we will group country 'UK' as 1 and 'Others' as 0.
5.  Also well be droping Description Coulmns as we already have Stockcode for product mapping also their are few Nan in Description Coulmns.


"""

df['Order Status']=df['InvoiceNo'].apply(lambda x: np.where(str(x)[0]=='C',0,1))                 #As there are some Canceled Data in the Transaction well be marking them as '0'.
print(df['Order Status'].value_counts())                                                         #Counting the Number of Canceled Orders.
try:
  df.drop('Description',axis=1,inplace=True)
  df.dropna(inplace=True)
except:
  pass
df['Country']=df['Country'].apply(lambda x: np.where(x=='United Kingdom','1','0'))
df.head()

df.describe()                                                                   #their is Negative Values in Quantity lets explore this column

df[df.Quantity<=0].head()                                                        #these are all Canceled Transanctions thus products are returned we can now remove InvoiceNo as we have saved that feature in order status
try:                                                    
  df.drop('InvoiceNo',axis=1,inplace=True)
except:
  pass

df['Temp_col']=df['StockCode'].apply(lambda x: np.where(len(str(x))==5,1,0))    # As stock code be a number of 5 digit we should look at the anomalies over here
print(df.Temp_col.value_counts())
df[df['Temp_col']==0].head()                                                    # Nothing conclusive found thus i will retain these values for futher and Dropping the temp column
try:
  df.drop('Temp_col',axis=1,inplace=True)
except:
  pass

print(type(df['InvoiceDate'][0]))                                               #Time stamp format
max_date=df['InvoiceDate'].max()
print(max_date)                                                                 #Lets take is date as a refrence for the Last Transaction, thus we will now compute the Last date of each transaction 
print(df['InvoiceDate'].min())
df['Days Before Last Trans']=df['InvoiceDate'].apply(lambda x: (max_date-x).days )

try:
  df.drop('InvoiceDate',axis=1,inplace=True)                                    # Dropping Invoice as it is of no use now
except:
  pass
df.head()

df['Amount']=df['Quantity']*df['UnitPrice']                                     #Multiplying Qnt and Price per unit to get Amount, and then Droping UnitPrice Column, Using Quantity i will make another Feature later
try:
  df.drop(['UnitPrice'],axis=1,inplace=True)
except:
  pass
df.head()

one_hot_entity=['Order_Status']    
column_one_hot=['Order Status']                                                 #One hot encoding of Order Status so as to get the gist of number of successful, canceled orders
count=0
for i in column_one_hot:
  temp_df=pd.get_dummies(df[i], prefix=one_hot_entity[count])
  count+=1
  df=pd.concat([df, temp_df], axis=1)
try:
  df.drop(['Order Status','StockCode'],axis=1,inplace=True)                     #Dropping Order Status and StockCode as they are of no use now
except:
  pass
df.head()

df1=df.groupby('CustomerID')['Country'].max()                                   # Our Features are now ready and thus i will be making the Final Dataset
df2=df.groupby('CustomerID')['Order_Status_0'].sum()
df3=df.groupby('CustomerID')['Order_Status_1'].sum()
df4=df.groupby('CustomerID')['Days Before Last Trans'].min()
df5=df.groupby('CustomerID')['Amount'].mean()
df6=df.groupby('CustomerID')['Amount'].sum()
df7=df.groupby('CustomerID')['Quantity'].mean()
columns=['Order_Status_0','Order_Status_1','Days Before Last Trans','Avg Amount','Total Amount','Avg_Quantity']
data_frames=[df2,df3,df4,df5,df6,df7]
final_df=pd.DataFrame(df1)
count=0
for i in columns:
  final_df[i]=data_frames[count]
  count+=1

print(final_df.shape)
final_df.head()                                                                 #Our Final Dataset is Completely Ready

print(np.percentile(final_df.Avg_Quantity, 98))                                 
print(np.percentile(final_df.Avg_Quantity, 99))                                 
print(np.percentile(final_df.Avg_Quantity, 99.5))                               #their is a sudden increase in the value after this
print(np.percentile(final_df.Avg_Quantity, 99.9))
print(np.percentile(final_df.Avg_Quantity, 99.99))
print(np.percentile(final_df.Avg_Quantity, 99.999))                             #Customer ordering more than 386 Quantity on avg is therefor a wholesaler and i will cluster them own my own

df_wholesalers=final_df[final_df.Avg_Quantity>=386]
final_df=final_df[final_df.Avg_Quantity<386]
final_df.head()

print(df_wholesalers.shape)
df_wholesalers.head()

def plot_scatter(final_df):
  fig = plt.figure(figsize = (20, 10))
  ax = plt.axes(projection ="3d")
  # Creating color map
  my_cmap = plt.get_cmap('hsv')
  plot=ax.scatter3D(final_df['Total Amount'],final_df['Days Before Last Trans'],final_df.Order_Status_1,cmap = my_cmap,alpha = 0.8,c =(final_df.Order_Status_1+final_df['Total Amount']+final_df['Days Before Last Trans']) )
  plt.title("3D scatter plot")
  ax.set_zlabel('Order Frequency', fontweight ='bold')
  ax.set_ylabel('Total Amount', fontweight ='bold')
  ax.set_xlabel('Recency in Order', fontweight ='bold')
  fig.colorbar(plot, ax = ax, aspect = 5)
  plt.show()       
                                                                              
plot_scatter(final_df)

minmax = MinMaxScaler(feature_range=(0, 1))
X = minmax.fit_transform(final_df[['Avg Amount','Total Amount','Avg_Quantity']])
clf = IsolationForest(n_estimators=100, contamination=0.05, random_state=0)     #Isolation Forest algorithm for anomaly detection
clf.fit(X)
print(final_df.shape)
final_df['multivariate_anomaly_score'] = clf.decision_function(X)               # predict raw anomaly score
final_df['multivariate_outlier'] = clf.predict(X)                               # prediction of a datapoint category outlier or inlier
final_df=final_df[final_df.multivariate_outlier==1]                             #updating data
print(final_df.shape)                                                           #outliers have been removed
final_df.drop(['multivariate_anomaly_score','multivariate_outlier'],axis=1,inplace=True)
plot_scatter(final_df)

def plot_dist(final_df,fig_size=(30,10),plot_type=0):
  columns=['Order_Status_0','Order_Status_1','Days Before Last Trans','Avg Amount','Total Amount','Avg_Quantity']
  fig,ax=plt.subplots(2,3,figsize=fig_size)
  count=0
  counter=0
  for i in columns:
    if plot_type==0:
      sns.distplot(final_df[i],ax=ax[count,counter])
    else:
      sns.countplot(final_df[i],ax=ax[count,counter])
    counter+=1
    if counter==3:
      counter=0
      count=1
    plt.show
plot_dist(final_df)

a = final_df.corr()
plt.figure(figsize=(30,10))
sns.heatmap(a,vmin=-1,vmax=1,center=0,annot=True)           #Co-raltion is high between few independent variable, thus their is a possibility that we will have to do pca for feature extraction and dimensionality reduction

minmax = MinMaxScaler(feature_range=(0, 1))
X = minmax.fit_transform(final_df[['Order_Status_0','Order_Status_1','Days Before Last Trans','Avg Amount','Total Amount','Avg_Quantity']])
def silhouette_score_analysis(n): 
  for n_clusters in range(2,n):
      km = KMeans (n_clusters=n_clusters, max_iter=100,tol=0.01)
      preds = km.fit_predict(X)
      centers = km.cluster_centers_
      score = silhouette_score(X, preds, metric='euclidean')
      print ("For n_clusters = {}, silhouette score is {}".format(n_clusters, score))
      visualizer = SilhouetteVisualizer(km)
      visualizer.fit(X) 
      visualizer.poof() 
silhouette_score_analysis(6)                                                    # More than 5 Clusters would Not seem Logical for Buisness Point of view.

"""As per Silhouette_score number of Cluster Could be 2 or 3"""

no_of_clusters = range(2,10)
inertia=[]
for f in no_of_clusters:
    kmeans = KMeans(n_clusters=f, random_state=2)
    kmeans = kmeans.fit(X)
    u = kmeans.inertia_
    inertia.append(u)
    print("The innertia for :", f, "Clusters is:", u)
fig, (ax1) = plt.subplots(1, figsize=(16,6))
xx = np.arange(len(no_of_clusters))
ax1.plot(xx, inertia)
ax1.set_xticks(xx)
ax1.set_xticklabels(no_of_clusters, rotation='vertical')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia Score')
plt.title("Inertia Plot per k")                                                 # But As per innertia plot clusters=3.

kmeans = KMeans(n_clusters=3, random_state=2) 
kmeans = kmeans.fit(X)
kmeans.labels_
predictions = kmeans.predict(X)
unique, counts = np.unique(predictions, return_counts=True)
counts = counts.reshape(1,3)
countscldf = pd.DataFrame(counts, columns = ["Cluster 0","Cluster 1","Cluster 2"])
countscldf

def pca_val(X,predictions): 
  X = X
  y_num = predictions
  pca = PCA(n_components=3, random_state =1 )
  X_r = pca.fit(X).transform(X)
  return X_r,y_num,pca
def plot_3d(X_r,y_num,pca):
  target_names = ["Cluster 0","Cluster 1","Cluster 2", "Cluster 3"]
  print('Explained variance ratio (first two components): %s' % str(pca.explained_variance_ratio_))
  plt.figure()
  plt.figure(figsize=(12,8))
  colors = ['navy', 'turquoise', 'darkorange', 'red']
  lw = 2
  ax = plt.axes(projection ="3d")

  for color, i, target_name in zip(colors, [0, 1, 2, 3,4], target_names):
      ax.scatter3D(X_r[y_num == i, 0], X_r[y_num == i, 1],X_r[y_num == i, 2], color=color, alpha=.8, lw=lw,label=target_name)
      
  plt.legend(loc='best', shadow=False, scatterpoints=1)
  plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.6)   
  plt.title('PCA of 2 Items')
  plt.show()                                                                     #these 3 components represents 95% Variance in data
X_r,y_num,pca=pca_val(X,predictions)
plot_3d(X_r,y_num,pca)

"""Clusters Looks a bit messed up (might be because there was a little co-relation between the independent variables), we will now be Doing PCA to get better results"""

n=6
for n_clusters in range(2,n):
    km = KMeans (n_clusters=n_clusters, max_iter=100,tol=0.01)
    preds = km.fit_predict(X_r)
    centers = km.cluster_centers_
    score = silhouette_score(X_r, preds, metric='euclidean')
    print ("For n_clusters = {}, silhouette score is {}".format(n_clusters, score))
    visualizer = SilhouetteVisualizer(km)
    visualizer.fit(X_r) 
    visualizer.poof()                                                           #on Pca as well number of Clusters perform Best

km = KMeans(n_clusters=4, max_iter=100,tol=0.01)                               #finally predicting Clusters for K=4
preds = km.fit_predict(X_r)

X_r,y_num,pca=pca_val(X,preds)
plot_3d(X_r,y_num,pca)                                                              #Explains 98% of Variance

final_df['Cluster']=preds                                                       #Loading the Cluster Values on Dataset
final_df.head()

dendrogram = sch.dendrogram(sch.linkage(X_r, method = 'ward'))                   #Doing Hierarchical Clustering and plotting a Dendogram to Visualize the Perfect value of K
plt.axhline(y=8, color='r', linestyle='--')
plt.title('Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Euclidean Distances')
plt.show()                                                                      # 4 Cluster looks Good

"""Now, that we have 5 customer Segments ( 4 from Kmeans Clusters and 1 of wholesalers), Lets Define them"""

Cluster_0=final_df[final_df['Cluster']==0]
Cluster_1=final_df[final_df['Cluster']==1]   
Cluster_2=final_df[final_df['Cluster']==2] 
Cluster_3=final_df[final_df['Cluster']==3] 
print(f'Number of Customers in Segment 1 = {Cluster_0.shape[0]}')
plot_dist(Cluster_0,fig_size=(30,8))
Cluster_0.head()

"""Customer Segment 1: Active Customers with medium buying frequency and Avg Transaction Values. """

print(f'Number of Customers in Segment 2 = {Cluster_1.shape[0]}')
plot_dist(Cluster_1,fig_size=(30,8))
Cluster_1.head()

"""Customer Segment 2: Passive Customers with low buying Frequency and medium transaction Values"""

print(f'Number of Customers in Segment 3 = {Cluster_2.shape[0]}')
plot_dist(Cluster_2,fig_size=(30,8))
Cluster_2.head()

"""Customer Segment 3: Passive Customers with low buying Frequency and transaction Values"""

print(f'Number of Customers in Segment 4 = {Cluster_3.shape[0]}')
plot_dist(Cluster_3,fig_size=(30,8))
Cluster_3.head()

"""Customer Segment 4: Active Customers with high Order frequency and Tansaction Value"""

print(f'Number of Customers in Segment 5 = {df_wholesalers.shape[0]}')
plot_dist(df_wholesalers,fig_size=(30,8))
df_wholesalers.head()

"""Customer Segment 5(wholesalers): Active Customers with low order frequency, high Total Amounts and Really high Quantities Order

We are done!! Customers are Segmented in 5 Different Groups, based on their behaviours, hope company would Target each customer accordingly now and the buisness would grow.
"""